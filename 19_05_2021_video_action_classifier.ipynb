{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19_05_2021_video_action_classifier.ipynb","provenance":[],"authorship_tag":"ABX9TyM3Q40n2gGQvajcb9DKRorL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJNu21KkwxCn","executionInfo":{"status":"ok","timestamp":1621370643497,"user_tz":-330,"elapsed":2198,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"6c82870d-fc6d-40c5-c6c0-3d0b7c5b47d4"},"source":["!git clone https://github.com/eriklindernoren/Action-Recognition.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Action-Recognition'...\n","remote: Enumerating objects: 51, done.\u001b[K\n","remote: Total 51 (delta 0), reused 0 (delta 0), pack-reused 51\u001b[K\n","Unpacking objects: 100% (51/51), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3xwUJKfsSkuW","executionInfo":{"status":"ok","timestamp":1621371808597,"user_tz":-330,"elapsed":1302,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}}},"source":["from google.colab import drive"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAmWovwTSnic","executionInfo":{"status":"ok","timestamp":1621372149850,"user_tz":-330,"elapsed":33731,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"f048afe9-4490-4c1d-b2d9-eb0068c7c7a9"},"source":["drive.mount('/content/gdrive/')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gS7My7-TVhfu","executionInfo":{"status":"ok","timestamp":1621372587425,"user_tz":-330,"elapsed":8518,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"46528f58-df72-4931-ef8e-19be3ba19de8"},"source":["!pip install av"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Collecting av\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/ff/bacde7314c646a2bd2f240034809a10cc3f8b096751284d0828640fff3dd/av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2MB)\n","\u001b[K     |████████████████████████████████| 37.2MB 1.2MB/s \n","\u001b[?25hInstalling collected packages: av\n","Successfully installed av-8.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buTLAe3mVtqb","executionInfo":{"status":"ok","timestamp":1621372632779,"user_tz":-330,"elapsed":4079,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"f2decd25-424e-4e22-d428-0916fafc6edf"},"source":["!pip install sk-video"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Collecting sk-video\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/3f/ce848b8b2062ad1ccf1449094a740c775f6c761339f411e44f1e090f23a7/sk_video-1.1.10-py2.py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 26.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.4.1)\n","Installing collected packages: sk-video\n","Successfully installed sk-video-1.1.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okezazxiUafh","executionInfo":{"status":"ok","timestamp":1621372737756,"user_tz":-330,"elapsed":2781,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"015f6bb6-3e03-4c6d-98c3-e75f9bad5917"},"source":["!python /content/Action-Recognition/test_on_video.py  --video_path /content/person01_boxing_d1_uncomp.avi \\\n","                            --checkpoint_model /content/gdrive/MyDrive/WEIGHTS/ConvLSTM_150.pth"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Namespace(channels=3, checkpoint_model='/content/gdrive/MyDrive/WEIGHTS/ConvLSTM_150.pth', image_dim=112, latent_dim=512, video_path='/content/person01_boxing_d1_uncomp.avi')\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","Traceback (most recent call last):\n","  File \"/content/Action-Recognition/test_on_video.py\", line 38, in <module>\n","    labels = sorted(list(set(os.listdir(opt.dataset_path))))\n","AttributeError: 'Namespace' object has no attribute 'dataset_path'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6W5TtTVGWTZz"},"source":["from models import *\n","from dataset import *\n","from data.extract_frames import extract_frames\n","import argparse\n","import os\n","import glob\n","import tqdm\n","from torchvision.utils import make_grid\n","from PIL import Image, ImageDraw\n","import skvideo.io\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"--video_path\", type=str, default=\"data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi\", help=\"Path to video\"\n","    )\n","    #parser.add_argument(\"--dataset_path\", type=str, default=\"data/UCF-101-frames\", help=\"Path to UCF-101 dataset\")\n","    parser.add_argument(\"--image_dim\", type=int, default=112, help=\"Height / width dimension\")\n","    parser.add_argument(\"--channels\", type=int, default=3, help=\"Number of image channels\")\n","    parser.add_argument(\"--latent_dim\", type=int, default=512, help=\"Dimensionality of the latent representation\")\n","    parser.add_argument(\"--checkpoint_model\", type=str, default=\"\", help=\"Optional path to checkpoint model\")\n","    opt = parser.parse_args()\n","    print(opt)\n","\n","    assert opt.checkpoint_model, \"Specify path to checkpoint model using arg. '--checkpoint_model'\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    input_shape = (opt.channels, opt.image_dim, opt.image_dim)\n","\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(input_shape[-2:], Image.BICUBIC),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","        ]\n","    )\n","\n","    labels = sorted(list(set(os.listdir(opt.dataset_path))))\n","\n","    # Define model and load model checkpoint\n","    model = ConvLSTM(input_shape=input_shape, num_classes=len(labels), latent_dim=opt.latent_dim)\n","    model.to(device)\n","    model.load_state_dict(torch.load(opt.checkpoint_model))\n","    model.eval()\n","\n","    # Extract predictions\n","    output_frames = []\n","    for frame in tqdm.tqdm(extract_frames(opt.video_path), desc=\"Processing frames\"):\n","        image_tensor = Variable(transform(frame)).to(device)\n","        image_tensor = image_tensor.view(1, 1, *image_tensor.shape)\n","\n","        # Get label prediction for frame\n","        with torch.no_grad():\n","            prediction = model(image_tensor)\n","            predicted_label = labels[prediction.argmax(1).item()]\n","\n","        # Draw label on frame\n","        d = ImageDraw.Draw(frame)\n","        d.text(xy=(10, 10), text=predicted_label, fill=(255, 255, 255))\n","\n","        output_frames += [frame]\n","\n","    # Create video from frames\n","    writer = skvideo.io.FFmpegWriter(\"output.gif\")\n","    for frame in tqdm.tqdm(output_frames, desc=\"Writing to video\"):\n","        writer.writeFrame(np.array(frame))\n","    writer.close()\n"],"execution_count":null,"outputs":[]}]}