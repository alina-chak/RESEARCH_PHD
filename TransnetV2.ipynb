{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransnetV2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMgPhL//FTNRgELPca5b5oj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLkGcZbXkns3","executionInfo":{"status":"ok","timestamp":1635013387913,"user_tz":-330,"elapsed":1129,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"d149ef66-939f-4d67-88db-b47632b7feeb"},"source":["!git clone https://github.com/soCzech/TransNetV2.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'TransNetV2'...\n","remote: Enumerating objects: 362, done.\u001b[K\n","remote: Counting objects: 100% (362/362), done.\u001b[K\n","remote: Compressing objects: 100% (225/225), done.\u001b[K\n","remote: Total 362 (delta 210), reused 268 (delta 119), pack-reused 0\u001b[K\n","Receiving objects: 100% (362/362), 96.03 KiB | 8.73 MiB/s, done.\n","Resolving deltas: 100% (210/210), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVb-qdU9k9PD","executionInfo":{"status":"ok","timestamp":1635012168979,"user_tz":-330,"elapsed":482,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"bc8ebb6b-a19f-4992-a098-d1589869b9ef"},"source":["%cd /content/TransNetV2/inference"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/TransNetV2/inference\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xp_ITwt2lEA8","executionInfo":{"status":"ok","timestamp":1635012217936,"user_tz":-330,"elapsed":458,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"7e0c044c-5f3f-4f98-8f2a-9c5a93b05e32"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dockerfile  __init__.py  README.md  transnetv2.py  transnetv2-weights\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0_8xyRwlG9z","executionInfo":{"status":"ok","timestamp":1635013697637,"user_tz":-330,"elapsed":87739,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"caae89e2-2b9d-4de8-f361-2a8a7908571f"},"source":["!pip install tensorflow==2.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.1\n","  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n","\u001b[K     |████████████████████████████████| 421.8 MB 23 kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.19.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (3.3.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.41.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (0.12.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (3.17.3)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 65.3 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (0.2.0)\n","Collecting tensorboard<2.2.0,>=2.1.0\n","  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 75.3 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (0.37.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.15.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (1.4.1)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1) (0.8.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1) (3.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.3.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1) (1.5.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=14488c1cec4cb3ef93a8cbb273c8e7045cb641dc2266d91b8c749cfe2f399bb2\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.6.0\n","    Uninstalling tensorflow-estimator-2.6.0:\n","      Successfully uninstalled tensorflow-estimator-2.6.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.6.0\n","    Uninstalling tensorboard-2.6.0:\n","      Successfully uninstalled tensorboard-2.6.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.6.0\n","    Uninstalling tensorflow-2.6.0:\n","      Successfully uninstalled tensorflow-2.6.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPvwF3gtrrcI","executionInfo":{"status":"ok","timestamp":1635013934944,"user_tz":-330,"elapsed":7200,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"ae6af520-15bb-4de2-8df7-d0907d7c3412"},"source":["!apt-get install ffmpeg\n","!pip install ffmpeg-python pillow"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n","Collecting ffmpeg-python\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n","Installing collected packages: ffmpeg-python\n","Successfully installed ffmpeg-python-0.2.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_sDDdaFlvMm","executionInfo":{"status":"ok","timestamp":1635012771781,"user_tz":-330,"elapsed":21704,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"ac0a5ace-1760-407e-e2af-14c2e8420bff"},"source":["!python transnetv2.py /content/v_Biking_g02_c02.avi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-10-23 18:12:34.409452: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2021-10-23 18:12:34.409602: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2021-10-23 18:12:34.409626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","[TransNetV2] Using weights from transnetv2-weights/.\n","2021-10-23 18:12:35.859162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-10-23 18:12:35.872666: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-10-23 18:12:35.872736: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (851512a61d13): /proc/driver/nvidia/version does not exist\n","2021-10-23 18:12:35.873089: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-10-23 18:12:35.878827: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200210000 Hz\n","2021-10-23 18:12:35.879100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a8e4a2680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-10-23 18:12:35.879140: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","[TransNetV2] Extracting frames from /content/v_Biking_g02_c02.avi\n","[TransNetV2] Processing video frames 240/240\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXBmLmXzYz-w","executionInfo":{"status":"ok","timestamp":1630399598797,"user_tz":-330,"elapsed":686,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"337404ce-1e73-40d6-8953-d775f9942691"},"source":["%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/TransNetV2\n"]}]},{"cell_type":"code","metadata":{"id":"Rwx4NR4VuUYx"},"source":["#CODE FOR EXTRACTING VIDEO FRAMES\n","import cv2\n","\n","# Opens the Video file\n","cap= cv2.VideoCapture('/content/TransNetV2/inference/v1.mp4')\n","i=0\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","    if ret == False:\n","        break\n","    cv2.imwrite('kang'+str(i)+'.jpg',frame)\n","    i+=1\n","\n","cap.release()\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXWaUoQ8bwfN","executionInfo":{"status":"ok","timestamp":1635014089020,"user_tz":-330,"elapsed":439,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"a94fe351-ed34-4d47-c1fe-c01c9393b713"},"source":["%cd /content/TransNetV2/inference"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/TransNetV2/inference\n"]}]},{"cell_type":"code","metadata":{"id":"kWcgBxu8byA7"},"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","class TransNetV2:\n","\n","    def __init__(self, model_dir=None):\n","        if model_dir is None:\n","            model_dir = os.path.join(os.path.dirname(__file__), \"transnetv2-weights/\")\n","            if not os.path.isdir(model_dir):\n","                raise FileNotFoundError(f\"[TransNetV2] ERROR: {model_dir} is not a directory.\")\n","            else:\n","                print(f\"[TransNetV2] Using weights from {model_dir}.\")\n","\n","        self._input_size = (27, 48, 3)\n","        try:\n","            self._model = tf.saved_model.load(model_dir)\n","        except OSError as exc:\n","            raise IOError(f\"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. \"\n","                          f\"Re-download them manually and retry. For more info, see: \"\n","                          f\"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796\") from exc\n","\n","    def predict_raw(self, frames: np.ndarray):\n","        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [batch, frames, height, width, 3].\"\n","        frames = tf.cast(frames, tf.float32)\n","\n","        logits, dict_ = self._model(frames)\n","        single_frame_pred = tf.sigmoid(logits)\n","        all_frames_pred = tf.sigmoid(dict_[\"many_hot\"])\n","\n","        return single_frame_pred, all_frames_pred\n","\n","    def predict_frames(self, frames: np.ndarray):\n","        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [frames, height, width, 3].\"\n","\n","        def input_iterator():\n","            # return windows of size 100 where the first/last 25 frames are from the previous/next batch\n","            # the first and last window must be padded by copies of the first and last frame of the video\n","            no_padded_frames_start = 25\n","            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74\n","\n","            start_frame = np.expand_dims(frames[0], 0)\n","            end_frame = np.expand_dims(frames[-1], 0)\n","            padded_inputs = np.concatenate(\n","                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0\n","            )\n","\n","            ptr = 0\n","            while ptr + 100 <= len(padded_inputs):\n","                out = padded_inputs[ptr:ptr + 100]\n","                ptr += 50\n","                yield out[np.newaxis]\n","\n","        predictions = []\n","\n","        for inp in input_iterator():\n","            single_frame_pred, all_frames_pred = self.predict_raw(inp)\n","            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],\n","                                all_frames_pred.numpy()[0, 25:75, 0]))\n","\n","            print(\"\\r[TransNetV2] Processing video frames {}/{}\".format(\n","                min(len(predictions) * 50, len(frames)), len(frames)\n","            ), end=\"\")\n","        print(\"\")\n","\n","        single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])\n","        all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])\n","\n","        return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames\n","\n","    def predict_video(self, video_fn: str):\n","        try:\n","            import ffmpeg\n","        except ModuleNotFoundError:\n","            raise ModuleNotFoundError(\"For `predict_video` function `ffmpeg` needs to be installed in order to extract \"\n","                                      \"individual frames from video file. Install `ffmpeg` command line tool and then \"\n","                                      \"install python wrapper by `pip install ffmpeg-python`.\")\n","\n","        print(\"[TransNetV2] Extracting frames from {}\".format(video_fn))\n","        video_stream, err = ffmpeg.input(video_fn).output(\n","            \"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\", s=\"48x27\"\n","        ).run(capture_stdout=True, capture_stderr=True)\n","\n","        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])\n","        return (video, *self.predict_frames(video))\n","\n","    @staticmethod\n","    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):\n","        predictions = (predictions > threshold).astype(np.uint8)\n","\n","        scenes = []\n","        t, t_prev, start = -1, 0, 0\n","        for i, t in enumerate(predictions):\n","            if t_prev == 1 and t == 0:\n","                start = i\n","            if t_prev == 0 and t == 1 and i != 0:\n","                scenes.append([start, i])\n","            t_prev = t\n","        if t == 0:\n","            scenes.append([start, i])\n","\n","        # just fix if all predictions are 1\n","        if len(scenes) == 0:\n","            return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n","\n","        return np.array(scenes, dtype=np.int32)\n","\n","    @staticmethod\n","    def visualize_predictions(frames: np.ndarray, predictions):\n","        from PIL import Image, ImageDraw\n","\n","        if isinstance(predictions, np.ndarray):\n","            predictions = [predictions]\n","\n","        ih, iw, ic = frames.shape[1:]\n","        width = 25\n","\n","        # pad frames so that length of the video is divisible by width\n","        # pad frames also by len(predictions) pixels in width in order to show predictions\n","        pad_with = width - len(frames) % width if len(frames) % width != 0 else 0\n","        frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)), (0, 0)])\n","\n","        predictions = [np.pad(x, (0, pad_with)) for x in predictions]\n","        height = len(frames) // width\n","\n","        img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])\n","        img = np.concatenate(np.split(\n","            np.concatenate(np.split(img, height), axis=2)[0], width\n","        ), axis=2)[0, :-1]\n","\n","        img = Image.fromarray(img)\n","        draw = ImageDraw.Draw(img)\n","\n","        # iterate over all frames\n","        for i, pred in enumerate(zip(*predictions)):\n","            x, y = i % width, i // width\n","            x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1\n","\n","            # we can visualize multiple predictions per single frame\n","            for j, p in enumerate(pred):\n","                color = [0, 0, 0]\n","                color[(j + 1) % 3] = 255\n","\n","                value = round(p * (ih - 1))\n","                if value != 0:\n","                    draw.line((x + j, y, x + j, y - value), fill=tuple(color), width=1)\n","        return img\n","\n","\n","def main():\n","    import sys\n","    import argparse\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"files\", type=str, nargs=\"+\", help=\"path to video files to process\")\n","    parser.add_argument(\"--weights\", type=str, default=None,\n","                        help=\"path to TransNet V2 weights, tries to infer the location if not specified\")\n","    parser.add_argument('--visualize', action=\"store_true\",\n","                        help=\"save a png file with prediction visualization for each extracted video\")\n","    args = parser.parse_args()\n","\n","    model = TransNetV2(args.weights)\n","    for file in args.files:\n","        if os.path.exists(file + \".predictions.txt\") or os.path.exists(file + \".scenes.txt\"):\n","            print(f\"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. \"\n","                  f\"Skipping video {file}.\", file=sys.stderr)\n","            continue\n","\n","        video_frames, single_frame_predictions, all_frame_predictions = \\\n","            model.predict_video(file)\n","\n","        predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","        np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","        scenes = model.predictions_to_scenes(single_frame_predictions)\n","        np.savetxt(file + \".scenes.txt\", scenes, fmt=\"%d\")\n","\n","        if args.visualize:\n","            if os.path.exists(file + \".vis.png\"):\n","                print(f\"[TransNetV2] {file}.vis.png already exists. \"\n","                      f\"Skipping visualization of video {file}.\", file=sys.stderr)\n","                continue\n","\n","            pil_image = model.visualize_predictions(\n","                video_frames, predictions=(single_frame_predictions, all_frame_predictions))\n","            pil_image.save(file + \".vis.png\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvQaecttirUe","executionInfo":{"status":"ok","timestamp":1635015930569,"user_tz":-330,"elapsed":433,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"ac4ee7fc-4394-491b-8fcd-cf154b3bc8ff"},"source":["%cd /content/TransNetV2/inference"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/TransNetV2/inference\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvJLaTEhiu2t","executionInfo":{"status":"ok","timestamp":1635015933172,"user_tz":-330,"elapsed":6,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"fb9a3e3f-bea1-487f-cc89-24fe4f88919b"},"source":["!ls"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Dockerfile  __init__.py  README.md  transnetv2.py  transnetv2-weights\n"]}]},{"cell_type":"code","metadata":{"id":"i_dJBXV1cB1D","executionInfo":{"status":"ok","timestamp":1635016227881,"user_tz":-330,"elapsed":512,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}}},"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","\n","__file__ = \"/content/TransNetV2/inference/\"\n","class TransNetV2:\n","\n","    def __init__(self, model_dir=None):\n","        if model_dir is None:\n","            model_dir = os.path.join(os.path.dirname(__file__), \"transnetv2-weights/\")\n","            if not os.path.isdir(model_dir):\n","                raise FileNotFoundError(f\"[TransNetV2] ERROR: {model_dir} is not a directory.\")\n","            else:\n","                print(f\"[TransNetV2] Using weights from {model_dir}.\")\n","\n","        self._input_size = (27, 48, 3)\n","        try:\n","            self._model = tf.saved_model.load(model_dir)\n","        except OSError as exc:\n","            raise IOError(f\"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. \"\n","                          f\"Re-download them manually and retry. For more info, see: \"\n","                          f\"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796\") from exc\n","\n","    def predict_raw(self, frames: np.ndarray):\n","        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [batch, frames, height, width, 3].\"\n","        frames = tf.cast(frames, tf.float32)\n","\n","        logits, dict_ = self._model(frames)\n","        single_frame_pred = tf.sigmoid(logits)\n","        all_frames_pred = tf.sigmoid(dict_[\"many_hot\"])\n","\n","        return single_frame_pred, all_frames_pred\n","\n","    def predict_frames(self, frames: np.ndarray):\n","        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [frames, height, width, 3].\"\n","\n","        def input_iterator():\n","            # return windows of size 100 where the first/last 25 frames are from the previous/next batch\n","            # the first and last window must be padded by copies of the first and last frame of the video\n","            no_padded_frames_start = 25\n","            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74\n","\n","            start_frame = np.expand_dims(frames[0], 0)\n","            end_frame = np.expand_dims(frames[-1], 0)\n","            padded_inputs = np.concatenate(\n","                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0\n","            )\n","\n","            ptr = 0\n","            while ptr + 100 <= len(padded_inputs):\n","                out = padded_inputs[ptr:ptr + 100]\n","                ptr += 50\n","                yield out[np.newaxis]\n","\n","        predictions = []\n","\n","        for inp in input_iterator():\n","            single_frame_pred, all_frames_pred = self.predict_raw(inp)\n","            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],\n","                                all_frames_pred.numpy()[0, 25:75, 0]))\n","\n","            print(\"\\r[TransNetV2] Processing video frames {}/{}\".format(\n","                min(len(predictions) * 50, len(frames)), len(frames)\n","            ), end=\"\")\n","        print(\"\")\n","\n","        single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])\n","        all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])\n","\n","        return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames\n","\n","    def predict_video(self, video_fn: str):\n","        try:\n","            import ffmpeg\n","        except ModuleNotFoundError:\n","            raise ModuleNotFoundError(\"For `predict_video` function `ffmpeg` needs to be installed in order to extract \"\n","                                      \"individual frames from video file. Install `ffmpeg` command line tool and then \"\n","                                      \"install python wrapper by `pip install ffmpeg-python`.\")\n","\n","        print(\"[TransNetV2] Extracting frames from {}\".format(video_fn))\n","        video_stream, err = ffmpeg.input(video_fn).output(\n","            \"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\", s=\"48x27\"\n","        ).run(capture_stdout=True, capture_stderr=True)\n","\n","        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])\n","        return (video, *self.predict_frames(video))\n","\n","    @staticmethod\n","    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):\n","        predictions = (predictions > threshold).astype(np.uint8)\n","\n","        scenes = []\n","        t, t_prev, start = -1, 0, 0\n","        for i, t in enumerate(predictions):\n","            if t_prev == 1 and t == 0:\n","                start = i\n","            if t_prev == 0 and t == 1 and i != 0:\n","                scenes.append([start, i])\n","            t_prev = t\n","        if t == 0:\n","            scenes.append([start, i])\n","\n","        # just fix if all predictions are 1\n","        if len(scenes) == 0:\n","            return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n","\n","        return np.array(scenes, dtype=np.int32)\n","\n","    @staticmethod\n","    def visualize_predictions(frames: np.ndarray, predictions):\n","        from PIL import Image, ImageDraw\n","\n","        if isinstance(predictions, np.ndarray):\n","            predictions = [predictions]\n","\n","        ih, iw, ic = frames.shape[1:]\n","        width = 25\n","\n","        # pad frames so that length of the video is divisible by width\n","        # pad frames also by len(predictions) pixels in width in order to show predictions\n","        pad_with = width - len(frames) % width if len(frames) % width != 0 else 0\n","        frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)), (0, 0)])\n","\n","        predictions = [np.pad(x, (0, pad_with)) for x in predictions]\n","        height = len(frames) // width\n","\n","        img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])\n","        img = np.concatenate(np.split(\n","            np.concatenate(np.split(img, height), axis=2)[0], width\n","        ), axis=2)[0, :-1]\n","\n","        img = Image.fromarray(img)\n","        draw = ImageDraw.Draw(img)\n","\n","        # iterate over all frames\n","        for i, pred in enumerate(zip(*predictions)):\n","            x, y = i % width, i // width\n","            x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1\n","\n","            # we can visualize multiple predictions per single frame\n","            for j, p in enumerate(pred):\n","                color = [0, 0, 0]\n","                color[(j + 1) % 3] = 255\n","\n","                value = round(p * (ih - 1))\n","                if value != 0:\n","                    draw.line((x + j, y, x + j, y - value), fill=tuple(color), width=1)\n","        return img\n"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJ-rWRc_cT9Z","executionInfo":{"status":"ok","timestamp":1635016310931,"user_tz":-330,"elapsed":17311,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"6537557c-c8cb-4a93-d65f-b40c14df0a7a"},"source":["model = TransNetV2() \n","file = \"/content/v_Biking_g01_c01.avi\"\n","\n","#if os.path.exists(file + \".predictions.txt\") or os.path.exists(file + \".scenes.txt\"):\n","#    print(f\"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. \"\n","#          f\"Skipping video {file}.\", file=sys.stderr)\n","    #continue\n","\n","\n","video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(file)\n","\n","predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","#np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","scenes = model.predictions_to_scenes(single_frame_predictions)\n","#np.savetxt(file + \".scenes.txt\", scenes, fmt=\"%d\")\n"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[TransNetV2] Using weights from /content/TransNetV2/inference/transnetv2-weights/.\n","[TransNetV2] Extracting frames from /content/v_Biking_g01_c01.avi\n","[TransNetV2] Processing video frames 151/151\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"toAJLBFlkX2o","executionInfo":{"status":"ok","timestamp":1635016582050,"user_tz":-330,"elapsed":421,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"21538c23-8181-439d-e783-ba5bdc221eb6"},"source":["len(scenes)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pQtjELuAldjD","executionInfo":{"status":"ok","timestamp":1635016666116,"user_tz":-330,"elapsed":21418,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"68488614-b4f2-4c04-dfee-d1318677f147"},"source":["model = TransNetV2() \n","file = \"/content/v_Biking_g02_c01.avi\"\n","\n","#if os.path.exists(file + \".predictions.txt\") or os.path.exists(file + \".scenes.txt\"):\n","#    print(f\"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. \"\n","#          f\"Skipping video {file}.\", file=sys.stderr)\n","    #continue\n","\n","\n","video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(file)\n","\n","predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","#np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","scenes = model.predictions_to_scenes(single_frame_predictions)\n","#np.savetxt(file + \".scenes.txt\", scenes, fmt=\"%d\")\n"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["[TransNetV2] Using weights from /content/TransNetV2/inference/transnetv2-weights/.\n","[TransNetV2] Extracting frames from /content/v_Biking_g02_c01.avi\n","[TransNetV2] Processing video frames 240/240\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQ_mZffdlmsV","executionInfo":{"status":"ok","timestamp":1635016672008,"user_tz":-330,"elapsed":433,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"56cb8ae5-36a4-458d-cb04-becaf3a49c73"},"source":["len(scenes)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-18C3Z7nmCox","executionInfo":{"status":"ok","timestamp":1635016814166,"user_tz":-330,"elapsed":19451,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"69d1dc15-5df0-4c28-b06d-81d76b829ea6"},"source":["model = TransNetV2() \n","file = \"/content/v_Biking_g02_c02.avi\"\n","\n","#if os.path.exists(file + \".predictions.txt\") or os.path.exists(file + \".scenes.txt\"):\n","#    print(f\"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. \"\n","#          f\"Skipping video {file}.\", file=sys.stderr)\n","    #continue\n","\n","\n","video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(file)\n","\n","predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","#np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","scenes = model.predictions_to_scenes(single_frame_predictions)\n","#np.savetxt(file + \".scenes.txt\", scenes, fmt=\"%d\")\n"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["[TransNetV2] Using weights from /content/TransNetV2/inference/transnetv2-weights/.\n","[TransNetV2] Extracting frames from /content/v_Biking_g02_c02.avi\n","[TransNetV2] Processing video frames 240/240\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8YoI6NGmN8l","executionInfo":{"status":"ok","timestamp":1635016830733,"user_tz":-330,"elapsed":556,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"4fb3322c-4dca-49c3-af03-44c975acf77a"},"source":["len(scenes)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PAIGWeUgmk3R","executionInfo":{"status":"ok","timestamp":1635016962202,"user_tz":-330,"elapsed":405,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"04f53071-1401-487b-9aba-a703a9d5eae8"},"source":["scenes"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,  53],\n","       [ 54, 162],\n","       [163, 229],\n","       [230, 239]], dtype=int32)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"sEM0MnpesV1Q","executionInfo":{"status":"ok","timestamp":1635019744207,"user_tz":-330,"elapsed":459,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}}},"source":["def create_video_from_frames(frames, title, fps=30, fourcc=cv2.VideoWriter_fourcc('m','p','4','v')):\n","    img_array = []\n","    for filename in frames:\n","        img = cv2.imread(filename)\n","        height, width, layers = img.shape\n","        size = (width,height)\n","        img_array.append(img)\n","\n","    out = cv2.VideoWriter(title, fourcc, 30, size)\n","    for i in range(len(img_array)):\n","        out.write(img_array[i])\n","    out.release()\n"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdSfDYvMpGAp","executionInfo":{"status":"ok","timestamp":1635023341263,"user_tz":-330,"elapsed":19788,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"93f699ad-b6f1-48be-8119-5ed88a7e968f"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"axes.grid\"] = False\n","import glob\n","import cv2\n","import os\n","\n","from sklearn.cluster import KMeans, spectral_clustering\n","from sklearn.metrics import silhouette_score, pairwise_distances_argmin_min\n","from scipy.signal import find_peaks\n","\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","import tensorflow as tf\n","\n","\n","video_id = 'Biking'\n","video = '/content/v_Biking_g02_c01.avi'\n","video_capture = cv2.VideoCapture(video)\n","currentframe = 0\n","try: \n","  if not os.path.exists(f'data/frames/{video_id}'): \n","    os.makedirs(f'data/frames/{video_id}') \n","except OSError: \n","  print ('Error: Creating directory of data')\n","while(True): \n","  ret, frame = video_capture.read() \n","  if ret: \n","    name = f'data/frames/{video_id}/' + str(currentframe) + '.jpg'\n","    print ('Creating...' + name) \n","    cv2.imwrite(name, frame) \n","    currentframe += 1\n","  else: \n","    break\n","video_capture.release() \n","cv2.destroyAllWindows()\n","\n","model = TransNetV2() \n","#file = \"/content/v_Biking_g02_c02.avi\"\n","\n","#if os.path.exists(file + \".predictions.txt\") or os.path.exists(file + \".scenes.txt\"):\n","#    print(f\"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. \"\n","#          f\"Skipping video {file}.\", file=sys.stderr)\n","    #continue\n","\n","\n","video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(video)\n","\n","predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","#np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","scenes = model.predictions_to_scenes(single_frame_predictions)\n","#np.savetxt(file + \".scenes.txt\", scenes, fmt=\"%d\")\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating...data/frames/Biking/0.jpg\n","Creating...data/frames/Biking/1.jpg\n","Creating...data/frames/Biking/2.jpg\n","Creating...data/frames/Biking/3.jpg\n","Creating...data/frames/Biking/4.jpg\n","Creating...data/frames/Biking/5.jpg\n","Creating...data/frames/Biking/6.jpg\n","Creating...data/frames/Biking/7.jpg\n","Creating...data/frames/Biking/8.jpg\n","Creating...data/frames/Biking/9.jpg\n","Creating...data/frames/Biking/10.jpg\n","Creating...data/frames/Biking/11.jpg\n","Creating...data/frames/Biking/12.jpg\n","Creating...data/frames/Biking/13.jpg\n","Creating...data/frames/Biking/14.jpg\n","Creating...data/frames/Biking/15.jpg\n","Creating...data/frames/Biking/16.jpg\n","Creating...data/frames/Biking/17.jpg\n","Creating...data/frames/Biking/18.jpg\n","Creating...data/frames/Biking/19.jpg\n","Creating...data/frames/Biking/20.jpg\n","Creating...data/frames/Biking/21.jpg\n","Creating...data/frames/Biking/22.jpg\n","Creating...data/frames/Biking/23.jpg\n","Creating...data/frames/Biking/24.jpg\n","Creating...data/frames/Biking/25.jpg\n","Creating...data/frames/Biking/26.jpg\n","Creating...data/frames/Biking/27.jpg\n","Creating...data/frames/Biking/28.jpg\n","Creating...data/frames/Biking/29.jpg\n","Creating...data/frames/Biking/30.jpg\n","Creating...data/frames/Biking/31.jpg\n","Creating...data/frames/Biking/32.jpg\n","Creating...data/frames/Biking/33.jpg\n","Creating...data/frames/Biking/34.jpg\n","Creating...data/frames/Biking/35.jpg\n","Creating...data/frames/Biking/36.jpg\n","Creating...data/frames/Biking/37.jpg\n","Creating...data/frames/Biking/38.jpg\n","Creating...data/frames/Biking/39.jpg\n","Creating...data/frames/Biking/40.jpg\n","Creating...data/frames/Biking/41.jpg\n","Creating...data/frames/Biking/42.jpg\n","Creating...data/frames/Biking/43.jpg\n","Creating...data/frames/Biking/44.jpg\n","Creating...data/frames/Biking/45.jpg\n","Creating...data/frames/Biking/46.jpg\n","Creating...data/frames/Biking/47.jpg\n","Creating...data/frames/Biking/48.jpg\n","Creating...data/frames/Biking/49.jpg\n","Creating...data/frames/Biking/50.jpg\n","Creating...data/frames/Biking/51.jpg\n","Creating...data/frames/Biking/52.jpg\n","Creating...data/frames/Biking/53.jpg\n","Creating...data/frames/Biking/54.jpg\n","Creating...data/frames/Biking/55.jpg\n","Creating...data/frames/Biking/56.jpg\n","Creating...data/frames/Biking/57.jpg\n","Creating...data/frames/Biking/58.jpg\n","Creating...data/frames/Biking/59.jpg\n","Creating...data/frames/Biking/60.jpg\n","Creating...data/frames/Biking/61.jpg\n","Creating...data/frames/Biking/62.jpg\n","Creating...data/frames/Biking/63.jpg\n","Creating...data/frames/Biking/64.jpg\n","Creating...data/frames/Biking/65.jpg\n","Creating...data/frames/Biking/66.jpg\n","Creating...data/frames/Biking/67.jpg\n","Creating...data/frames/Biking/68.jpg\n","Creating...data/frames/Biking/69.jpg\n","Creating...data/frames/Biking/70.jpg\n","Creating...data/frames/Biking/71.jpg\n","Creating...data/frames/Biking/72.jpg\n","Creating...data/frames/Biking/73.jpg\n","Creating...data/frames/Biking/74.jpg\n","Creating...data/frames/Biking/75.jpg\n","Creating...data/frames/Biking/76.jpg\n","Creating...data/frames/Biking/77.jpg\n","Creating...data/frames/Biking/78.jpg\n","Creating...data/frames/Biking/79.jpg\n","Creating...data/frames/Biking/80.jpg\n","Creating...data/frames/Biking/81.jpg\n","Creating...data/frames/Biking/82.jpg\n","Creating...data/frames/Biking/83.jpg\n","Creating...data/frames/Biking/84.jpg\n","Creating...data/frames/Biking/85.jpg\n","Creating...data/frames/Biking/86.jpg\n","Creating...data/frames/Biking/87.jpg\n","Creating...data/frames/Biking/88.jpg\n","Creating...data/frames/Biking/89.jpg\n","Creating...data/frames/Biking/90.jpg\n","Creating...data/frames/Biking/91.jpg\n","Creating...data/frames/Biking/92.jpg\n","Creating...data/frames/Biking/93.jpg\n","Creating...data/frames/Biking/94.jpg\n","Creating...data/frames/Biking/95.jpg\n","Creating...data/frames/Biking/96.jpg\n","Creating...data/frames/Biking/97.jpg\n","Creating...data/frames/Biking/98.jpg\n","Creating...data/frames/Biking/99.jpg\n","Creating...data/frames/Biking/100.jpg\n","Creating...data/frames/Biking/101.jpg\n","Creating...data/frames/Biking/102.jpg\n","Creating...data/frames/Biking/103.jpg\n","Creating...data/frames/Biking/104.jpg\n","Creating...data/frames/Biking/105.jpg\n","Creating...data/frames/Biking/106.jpg\n","Creating...data/frames/Biking/107.jpg\n","Creating...data/frames/Biking/108.jpg\n","Creating...data/frames/Biking/109.jpg\n","Creating...data/frames/Biking/110.jpg\n","Creating...data/frames/Biking/111.jpg\n","Creating...data/frames/Biking/112.jpg\n","Creating...data/frames/Biking/113.jpg\n","Creating...data/frames/Biking/114.jpg\n","Creating...data/frames/Biking/115.jpg\n","Creating...data/frames/Biking/116.jpg\n","Creating...data/frames/Biking/117.jpg\n","Creating...data/frames/Biking/118.jpg\n","Creating...data/frames/Biking/119.jpg\n","Creating...data/frames/Biking/120.jpg\n","Creating...data/frames/Biking/121.jpg\n","Creating...data/frames/Biking/122.jpg\n","Creating...data/frames/Biking/123.jpg\n","Creating...data/frames/Biking/124.jpg\n","Creating...data/frames/Biking/125.jpg\n","Creating...data/frames/Biking/126.jpg\n","Creating...data/frames/Biking/127.jpg\n","Creating...data/frames/Biking/128.jpg\n","Creating...data/frames/Biking/129.jpg\n","Creating...data/frames/Biking/130.jpg\n","Creating...data/frames/Biking/131.jpg\n","Creating...data/frames/Biking/132.jpg\n","Creating...data/frames/Biking/133.jpg\n","Creating...data/frames/Biking/134.jpg\n","Creating...data/frames/Biking/135.jpg\n","Creating...data/frames/Biking/136.jpg\n","Creating...data/frames/Biking/137.jpg\n","Creating...data/frames/Biking/138.jpg\n","Creating...data/frames/Biking/139.jpg\n","Creating...data/frames/Biking/140.jpg\n","Creating...data/frames/Biking/141.jpg\n","Creating...data/frames/Biking/142.jpg\n","Creating...data/frames/Biking/143.jpg\n","Creating...data/frames/Biking/144.jpg\n","Creating...data/frames/Biking/145.jpg\n","Creating...data/frames/Biking/146.jpg\n","Creating...data/frames/Biking/147.jpg\n","Creating...data/frames/Biking/148.jpg\n","Creating...data/frames/Biking/149.jpg\n","Creating...data/frames/Biking/150.jpg\n","Creating...data/frames/Biking/151.jpg\n","Creating...data/frames/Biking/152.jpg\n","Creating...data/frames/Biking/153.jpg\n","Creating...data/frames/Biking/154.jpg\n","Creating...data/frames/Biking/155.jpg\n","Creating...data/frames/Biking/156.jpg\n","Creating...data/frames/Biking/157.jpg\n","Creating...data/frames/Biking/158.jpg\n","Creating...data/frames/Biking/159.jpg\n","Creating...data/frames/Biking/160.jpg\n","Creating...data/frames/Biking/161.jpg\n","Creating...data/frames/Biking/162.jpg\n","Creating...data/frames/Biking/163.jpg\n","Creating...data/frames/Biking/164.jpg\n","Creating...data/frames/Biking/165.jpg\n","Creating...data/frames/Biking/166.jpg\n","Creating...data/frames/Biking/167.jpg\n","Creating...data/frames/Biking/168.jpg\n","Creating...data/frames/Biking/169.jpg\n","Creating...data/frames/Biking/170.jpg\n","Creating...data/frames/Biking/171.jpg\n","Creating...data/frames/Biking/172.jpg\n","Creating...data/frames/Biking/173.jpg\n","Creating...data/frames/Biking/174.jpg\n","Creating...data/frames/Biking/175.jpg\n","Creating...data/frames/Biking/176.jpg\n","Creating...data/frames/Biking/177.jpg\n","Creating...data/frames/Biking/178.jpg\n","Creating...data/frames/Biking/179.jpg\n","Creating...data/frames/Biking/180.jpg\n","Creating...data/frames/Biking/181.jpg\n","Creating...data/frames/Biking/182.jpg\n","Creating...data/frames/Biking/183.jpg\n","Creating...data/frames/Biking/184.jpg\n","Creating...data/frames/Biking/185.jpg\n","Creating...data/frames/Biking/186.jpg\n","Creating...data/frames/Biking/187.jpg\n","Creating...data/frames/Biking/188.jpg\n","Creating...data/frames/Biking/189.jpg\n","Creating...data/frames/Biking/190.jpg\n","Creating...data/frames/Biking/191.jpg\n","Creating...data/frames/Biking/192.jpg\n","Creating...data/frames/Biking/193.jpg\n","Creating...data/frames/Biking/194.jpg\n","Creating...data/frames/Biking/195.jpg\n","Creating...data/frames/Biking/196.jpg\n","Creating...data/frames/Biking/197.jpg\n","Creating...data/frames/Biking/198.jpg\n","Creating...data/frames/Biking/199.jpg\n","Creating...data/frames/Biking/200.jpg\n","Creating...data/frames/Biking/201.jpg\n","Creating...data/frames/Biking/202.jpg\n","Creating...data/frames/Biking/203.jpg\n","Creating...data/frames/Biking/204.jpg\n","Creating...data/frames/Biking/205.jpg\n","Creating...data/frames/Biking/206.jpg\n","Creating...data/frames/Biking/207.jpg\n","Creating...data/frames/Biking/208.jpg\n","Creating...data/frames/Biking/209.jpg\n","Creating...data/frames/Biking/210.jpg\n","Creating...data/frames/Biking/211.jpg\n","Creating...data/frames/Biking/212.jpg\n","Creating...data/frames/Biking/213.jpg\n","Creating...data/frames/Biking/214.jpg\n","Creating...data/frames/Biking/215.jpg\n","Creating...data/frames/Biking/216.jpg\n","Creating...data/frames/Biking/217.jpg\n","Creating...data/frames/Biking/218.jpg\n","Creating...data/frames/Biking/219.jpg\n","Creating...data/frames/Biking/220.jpg\n","Creating...data/frames/Biking/221.jpg\n","Creating...data/frames/Biking/222.jpg\n","Creating...data/frames/Biking/223.jpg\n","Creating...data/frames/Biking/224.jpg\n","Creating...data/frames/Biking/225.jpg\n","Creating...data/frames/Biking/226.jpg\n","Creating...data/frames/Biking/227.jpg\n","Creating...data/frames/Biking/228.jpg\n","Creating...data/frames/Biking/229.jpg\n","Creating...data/frames/Biking/230.jpg\n","Creating...data/frames/Biking/231.jpg\n","Creating...data/frames/Biking/232.jpg\n","Creating...data/frames/Biking/233.jpg\n","Creating...data/frames/Biking/234.jpg\n","Creating...data/frames/Biking/235.jpg\n","Creating...data/frames/Biking/236.jpg\n","Creating...data/frames/Biking/237.jpg\n","Creating...data/frames/Biking/238.jpg\n","Creating...data/frames/Biking/239.jpg\n","[TransNetV2] Using weights from /content/TransNetV2/inference/transnetv2-weights/.\n","[TransNetV2] Extracting frames from /content/v_Biking_g02_c01.avi\n","[TransNetV2] Processing video frames 240/240\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyu9LJ9_q8t0","executionInfo":{"status":"ok","timestamp":1635023381492,"user_tz":-330,"elapsed":447,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}},"outputId":"54e9b357-92f9-4934-8c6f-5549a0eeb200"},"source":["category = \"Biking\"\n","sbd_frames = []\n","dir = \"/content/TransNetV2/inference/data/frames/\" + category\n","for i in range(0,len(scenes)):\n","  #print(scenes[i][1])\n","  sbd_frames.append(dir + \"/\" + str(scenes[i][0]) + \".jpg\")\n","  sbd_frames.append(dir + \"/\" + str(scenes[i][1]) + \".jpg\")\n","\n","#sbd_frames = np.array(sbd_frames)\n","print(sbd_frames)"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/TransNetV2/inference/data/frames/Biking/0.jpg', '/content/TransNetV2/inference/data/frames/Biking/194.jpg', '/content/TransNetV2/inference/data/frames/Biking/195.jpg', '/content/TransNetV2/inference/data/frames/Biking/239.jpg']\n"]}]},{"cell_type":"code","metadata":{"id":"mvg98NAKycV5","executionInfo":{"status":"ok","timestamp":1635020411094,"user_tz":-330,"elapsed":393,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}}},"source":["title = \"/content/TransNetV2/inference/dataset/new/Biking/b2.mp4\"\n","img_array = []\n","fourcc = cv2.VideoWriter_fourcc('m','p','4','v')\n","for filename in sbd_frames:\n","  img = cv2.imread(filename)\n","  height, width, layers = img.shape\n","  size = (width,height)\n","  img_array.append(img)\n","\n","out = cv2.VideoWriter(title, fourcc, 30, size)\n","for i in range(len(img_array)):\n","    out.write(img_array[i])\n","out.release()\n"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQrfWbLRzgge","executionInfo":{"status":"ok","timestamp":1635020427526,"user_tz":-330,"elapsed":397,"user":{"displayName":"Alina Banerjee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10819285316635491372"}}},"source":["shutil.rmtree(f'data/frames/{video_id}')"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbF62dmm0jo3"},"source":["#CODE FOR EXTRACTING PIXEL VALUES as features and storing as feature files all categories in one pickle file\n","\n","import os\n","import numpy as np\n","import tensorflow as tf\n","\n","__file__ = \"/content/TransNetV2/inference/\"\n","class TransNetV2:\n","\n","    def __init__(self, model_dir=None):\n","        if model_dir is None:\n","            model_dir = os.path.join(os.path.dirname(__file__), \"transnetv2-weights/\")\n","            if not os.path.isdir(model_dir):\n","                raise FileNotFoundError(f\"[TransNetV2] ERROR: {model_dir} is not a directory.\")\n","            else:\n","                print(f\"[TransNetV2] Using weights from {model_dir}.\")\n","\n","        self._input_size = (27, 48, 3)\n","        try:\n","            self._model = tf.saved_model.load(model_dir)\n","        except OSError as exc:\n","            raise IOError(f\"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. \"\n","                          f\"Re-download them manually and retry. For more info, see: \"\n","                          f\"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796\") from exc\n","\n","    def predict_raw(self, frames: np.ndarray):\n","        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [batch, frames, height, width, 3].\"\n","        frames = tf.cast(frames, tf.float32)\n","\n","        logits, dict_ = self._model(frames)\n","        single_frame_pred = tf.sigmoid(logits)\n","        all_frames_pred = tf.sigmoid(dict_[\"many_hot\"])\n","\n","        return single_frame_pred, all_frames_pred\n","\n","    def predict_frames(self, frames: np.ndarray):\n","        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \\\n","            \"[TransNetV2] Input shape must be [frames, height, width, 3].\"\n","\n","        def input_iterator():\n","            # return windows of size 100 where the first/last 25 frames are from the previous/next batch\n","            # the first and last window must be padded by copies of the first and last frame of the video\n","            no_padded_frames_start = 25\n","            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74\n","\n","            start_frame = np.expand_dims(frames[0], 0)\n","            end_frame = np.expand_dims(frames[-1], 0)\n","            padded_inputs = np.concatenate(\n","                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0\n","            )\n","\n","            ptr = 0\n","            while ptr + 100 <= len(padded_inputs):\n","                out = padded_inputs[ptr:ptr + 100]\n","                ptr += 50\n","                yield out[np.newaxis]\n","\n","        predictions = []\n","\n","        for inp in input_iterator():\n","            single_frame_pred, all_frames_pred = self.predict_raw(inp)\n","            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],\n","                                all_frames_pred.numpy()[0, 25:75, 0]))\n","\n","            print(\"\\r[TransNetV2] Processing video frames {}/{}\".format(\n","                min(len(predictions) * 50, len(frames)), len(frames)\n","            ), end=\"\")\n","        print(\"\")\n","\n","        single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])\n","        all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])\n","\n","        return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames\n","\n","    def predict_video(self, video_fn: str):\n","        try:\n","            import ffmpeg\n","        except ModuleNotFoundError:\n","            raise ModuleNotFoundError(\"For `predict_video` function `ffmpeg` needs to be installed in order to extract \"\n","                                      \"individual frames from video file. Install `ffmpeg` command line tool and then \"\n","                                      \"install python wrapper by `pip install ffmpeg-python`.\")\n","\n","        print(\"[TransNetV2] Extracting frames from {}\".format(video_fn))\n","        video_stream, err = ffmpeg.input(video_fn).output(\n","            \"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\", s=\"48x27\"\n","        ).run(capture_stdout=True, capture_stderr=True)\n","\n","        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])\n","        return (video, *self.predict_frames(video))\n","\n","    @staticmethod\n","    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):\n","        predictions = (predictions > threshold).astype(np.uint8)\n","\n","        scenes = []\n","        t, t_prev, start = -1, 0, 0\n","        for i, t in enumerate(predictions):\n","            if t_prev == 1 and t == 0:\n","                start = i\n","            if t_prev == 0 and t == 1 and i != 0:\n","                scenes.append([start, i])\n","            t_prev = t\n","        if t == 0:\n","            scenes.append([start, i])\n","\n","        # just fix if all predictions are 1\n","        if len(scenes) == 0:\n","            return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n","\n","        return np.array(scenes, dtype=np.int32)\n","\n","    @staticmethod\n","    def visualize_predictions(frames: np.ndarray, predictions):\n","        from PIL import Image, ImageDraw\n","\n","        if isinstance(predictions, np.ndarray):\n","            predictions = [predictions]\n","\n","        ih, iw, ic = frames.shape[1:]\n","        width = 25\n","\n","        # pad frames so that length of the video is divisible by width\n","        # pad frames also by len(predictions) pixels in width in order to show predictions\n","        pad_with = width - len(frames) % width if len(frames) % width != 0 else 0\n","        frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)), (0, 0)])\n","\n","        predictions = [np.pad(x, (0, pad_with)) for x in predictions]\n","        height = len(frames) // width\n","\n","        img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])\n","        img = np.concatenate(np.split(\n","            np.concatenate(np.split(img, height), axis=2)[0], width\n","        ), axis=2)[0, :-1]\n","\n","        img = Image.fromarray(img)\n","        draw = ImageDraw.Draw(img)\n","\n","        # iterate over all frames\n","        for i, pred in enumerate(zip(*predictions)):\n","            x, y = i % width, i // width\n","            x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1\n","\n","            # we can visualize multiple predictions per single frame\n","            for j, p in enumerate(pred):\n","                color = [0, 0, 0]\n","                color[(j + 1) % 3] = 255\n","\n","                value = round(p * (ih - 1))\n","                if value != 0:\n","                    draw.line((x + j, y, x + j, y - value), fill=tuple(color), width=1)\n","        return img\n","\n","\n","import cv2\n","import numpy as np\n","import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"axes.grid\"] = False\n","import glob\n","import cv2\n","import os\n","\n","from sklearn.cluster import KMeans, spectral_clustering\n","from sklearn.metrics import silhouette_score, pairwise_distances_argmin_min\n","from scipy.signal import find_peaks\n","\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","import tensorflow as tf\n","import shutil\n","\n","def sort_frames(list_of_frames, video_id):\n","    return sorted(list_of_frames, key=lambda x: int(x.split(f'data/frames/{video_id}/')[1].split('.jpg')[0]))\n","\n","def get_image_features(frames_filepaths):\n","  image_features = []\n","  model = VGG16(weights='imagenet', include_top=False)\n","  for img_path in frames_filepaths:\n","      print(img_path)\n","      img = image.load_img(img_path, target_size=(224, 224))\n","      img_data = image.img_to_array(img)\n","      img_data = np.expand_dims(img_data, axis=0)\n","      img_data = preprocess_input(img_data)\n","\n","      vgg16_feature = model.predict(img_data)\n","      image_features.append(np.array(vgg16_feature).flatten())\n","  return image_features\n","\n","def kmeans(image_features, n_clusters):\n","  silhouette = np.zeros(n_clusters)\n","  for k in range(2, n_clusters):\n","    kmeans = KMeans(n_clusters=k).fit_predict(image_features)\n","    silhouette[k] = silhouette_score(image_features, kmeans)\n","    print(f'{k} cluster complete')\n","  return silhouette\n","\n","def random_frame_idxs_for_cluster(kmeans_labels):\n","  idxs = []\n","  for label in range(len(np.unique(kmeans_labels))):\n","    idxs.append(np.random.choice(np.argwhere(kmeans_labels == label).flatten(), 1)[0])\n","  return np.sort(np.array(idxs))\n","\n","def closest_to_centroid_frames(kmeans, image_list, image_features):\n","  closest, _ = pairwise_distances_argmin_min(\n","      kmeans.cluster_centers_, image_features)\n","  return np.array(image_list)[closest]\n","\n","def create_video_from_frames(frames, title, fps=30, fourcc=cv2.VideoWriter_fourcc('m','p','4','v')):\n","    img_array = []\n","    for filename in frames:\n","        img = cv2.imread(filename)\n","        height, width, layers = img.shape\n","        size = (width,height)\n","        img_array.append(img)\n","\n","    out = cv2.VideoWriter(title, fourcc, 30, size)\n","    for i in range(len(img_array)):\n","        out.write(img_array[i])\n","    out.release()\n","\n","\n","\n","CATEGORIES = [\"Biking\", \"JumpingJack\", \"Kayaking\", \"PlayingGuitar\", \"Rowing\", \"Skijet\", \"TaiChi\"]#, \"handwaving\"]#, \"jogging\", \"running\", \n","   # \"walking\"]\n","\n","if __name__ == \"__main__\":\n","\n","    # Create directory to store extracted pixel features.\n","    os.makedirs(\"data\", exist_ok=True)\n","\n","    \n","    n_processed_files = 0\n","    features = []\n","\n","    for category in CATEGORIES:\n","        print(\"Processing category %s\" % category)\n","        cou = 0\n","        # Get all files in current category's folder.\n","        folder_path = os.path.join(\"..\", \"dataset\", category)\n","        filenames = os.listdir(folder_path)\n","\n","        # List to store features. features[i] stores features for the i-th video\n","        # in current category.\n","        #features = []\n","\n","        for filename in filenames:\n","            filepath = os.path.join(\"..\", \"dataset\", category, filename)\n","            path = \"/content/KTH-Action-Recognition/dataset/\" + category + \"/\" + filename\n","            video_id = category\n","            cou = cou + 1\n","            video = filepath\n","            video_capture = cv2.VideoCapture(video)\n","            currentframe = 0\n","            try: \n","              if not os.path.exists(f'data/frames/{video_id}'): \n","                os.makedirs(f'data/frames/{video_id}') \n","            except OSError: \n","              print ('Error: Creating directory of data')\n","            while(True): \n","              ret, frame = video_capture.read() \n","              if ret: \n","                name = f'data/frames/{video_id}/' + str(currentframe) + '.jpg'\n","                print ('Creating...' + name) \n","                cv2.imwrite(name, frame) \n","                currentframe += 1\n","              else: \n","                break\n","            video_capture.release() \n","            cv2.destroyAllWindows()\n","\n","\n","            model = TransNetV2() \n","\n","\n","            video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(video)\n","\n","            predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)\n","            #np.savetxt(file + \".predictions.txt\", predictions, fmt=\"%.6f\")\n","\n","            scenes = model.predictions_to_scenes(single_frame_predictions)\n","            sbd_frames = []\n","            dir = \"/content/TransNetV2/inference/data/frames/\" + category\n","            for i in range(0,len(scenes)):\n","              #print(scenes[i][1])\n","              sbd_frames.append(dir + \"/\" + str(scenes[i][0]) + \".jpg\")\n","              sbd_frames.append(dir + \"/\" + str(scenes[i][1]) + \".jpg\")\n","\n","            #sbd_frames = np.array(sbd_frames)\n","            print(sbd_frames)\n","\n","\n","            \n","            original_frames = sbd_frames\n","            try: \n","              if not os.path.exists(f'dataset/new/{video_id}'): \n","                os.makedirs(f'dataset/new/{video_id}')\n","                 \n","                \n","            except OSError: \n","              print ('Error: Creating directory of data')\n","            create_video_from_frames(original_frames, '/content/KTH-Action-Recognition/dataset/dataset/new/'+ video_id + '/' + category + str(cou)+'.mp4')\n","\n","            shutil.rmtree(f'data/frames/{video_id}')\n","\n","\n","            n_processed_files += 1\n","            if n_processed_files % 30 == 0:\n","                print(\"Done %d files\" % n_processed_files)\n","\n","        # Dump data to file.\n","#pickle.dump(features, open(\"data/pixel.p\" , \"wb\"))\n","\n","\n"],"execution_count":null,"outputs":[]}]}