{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"_Improved_SBD_Gygli.ipynb","provenance":[{"file_id":"1z9Wbu_0iUP0i7Qn-mSV9Qi7ZRDdNZx6R","timestamp":1613366047019}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2LuU7upaOoX","executionInfo":{"status":"ok","timestamp":1614668525713,"user_tz":-330,"elapsed":5375,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"fc20ae18-7df7-43a9-ed47-b6a2626b4dee"},"source":["#OPENCV VERSION FOR USING SIFT\r\n","!pip install opencv-python==3.4.2.17\r\n","!pip install opencv-contrib-python==3.4.2.17\r\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n","Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3LK5Wiw6ZUH","executionInfo":{"status":"ok","timestamp":1614663502139,"user_tz":-330,"elapsed":5153,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"00548a8a-9d52-4369-f5e8-3f04afc38655"},"source":["!git clone https://github.com/oladeha2/shot_boudary_detector.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'shot_boudary_detector'...\n","remote: Enumerating objects: 64, done.\u001b[K\n","remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n","Unpacking objects: 100% (64/64), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScEUbTMLoDHZ","executionInfo":{"status":"ok","timestamp":1614572327185,"user_tz":-330,"elapsed":3235,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"0a20a3d7-a56a-4414-f715-e6ac76ad4d50"},"source":["%cd /content/\r\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","sample_data  shot_boudary_detector\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lh11MGB57Xxe","executionInfo":{"status":"ok","timestamp":1614668532595,"user_tz":-330,"elapsed":1515,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"e27c1abd-06b1-4fc4-a7cd-eb80cd072264"},"source":["%cd /content/shot_boudary_detector\n","!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/shot_boudary_detector\n","get_transition_frames_cpu.py\t\tTestVideo.py\n","get_transition_frames_gpu.py\t\ttransition_network.py\n","README.md\t\t\t\tutilities.py\n","shot_boundary_detector_even_distrib.pt\tvideo_processing.py\n","snippet.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4wu_tdgvWxh","executionInfo":{"status":"ok","timestamp":1613242541117,"user_tz":-330,"elapsed":113708,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"09776e59-17b5-49d9-cf26-b7f188e60e5e"},"source":["!python image_save.py /content/v1.mp4"],"execution_count":null,"outputs":[{"output_type":"stream","text":["decomposing video to frames this may take a while  for large videos :) .....\n","final size:  (64, 64)\n","frame decomposition complete !!! \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ziaM8PFHSslf"},"source":["import sys\n","import shutil\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import numpy as np\n","from snippet import getSnippet\n","from math import floor\n","from transition_network import TransitionCNN\n","from utilities import normalize_frame, print_shape\n","import pandas as pd\n","import os\n","import time\n","from TestVideo import TestVideo, return_start_and_end\n","from moviepy.editor import VideoFileClip\n","from video_processing import six_four_crop_video\n","from PIL import Image\n","\n","# command line arguments --> file name, video_file_name, gpu or cpu \n","\n","\n","# first decompose the video to frames\n","# place the video to be detected into the directory \n","\n","video = sys.argv[1]\n","pred_text_file_name = sys.argv[2]\n","\n","\n","text_file = 'frames.txt'\n","\n","print('decomposing video to frames this may take a while  for large videos :) .....')\n","frames_path = 'video_frames/'\n","os.makedirs('video_frames/', exist_ok=True)\n","os.makedirs('predictions/', exist_ok=True)\n","\n","vid = VideoFileClip(video)\n","vid = six_four_crop_video(vid)\n","\n","frames = [frame for frame in vid.iter_frames()]\n","\n","f = open(text_file, 'w+')\n","\n","for j, frame in enumerate(frames):\n","        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\n","        im = Image.fromarray(frame)\n","        im.save(frame_path)            \n","        f.write(frame_path + '\\n')    \n","\n","print('frame decomposition complete !!! ')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rh0_Ve3zo5yr","executionInfo":{"status":"ok","timestamp":1613240997358,"user_tz":-330,"elapsed":1629,"user":{"displayName":"Alina Banerjee","photoUrl":"","userId":"10819285316635491372"}},"outputId":"4c1c2210-ff7d-4b28-d48e-2e8671a7d134"},"source":["#CODE FOR GETTING NO OF FRAMES\r\n","import cv2\r\n","cap= cv2.VideoCapture('/content/v1.mp4')\r\n","\r\n","totalframecount= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n","\r\n","print(\"The total number of frames in this video is \", totalframecount)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The total number of frames in this video is  14541\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_cl5wKQGJfIF"},"source":["#CODE FOR EXTRACTING VIDEO FRAMES\r\n","import cv2\r\n","\r\n","# Opens the Video file\r\n","cap= cv2.VideoCapture('/content/v1.mp4')\r\n","i=0\r\n","while(cap.isOpened()):\r\n","    ret, frame = cap.read()\r\n","    if ret == False:\r\n","        break\r\n","    cv2.imwrite('kang'+str(i)+'.jpg',frame)\r\n","    i+=1\r\n","\r\n","cap.release()\r\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PeRygES_KC0m"},"source":["#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL\r\n","import sys\r\n","import shutil\r\n","\r\n","import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from torchvision import transforms, utils\r\n","import numpy as np\r\n","from snippet import getSnippet\r\n","from math import floor\r\n","from transition_network import TransitionCNN\r\n","from utilities import normalize_frame, print_shape\r\n","import pandas as pd\r\n","import os\r\n","import time\r\n","from TestVideo import TestVideo, return_start_and_end\r\n","from moviepy.editor import VideoFileClip\r\n","from video_processing import six_four_crop_video\r\n","from PIL import Image\r\n","import cv2 \r\n","#import matplotlib.pyplot as plt\r\n","#%matplotlib inline\r\n","\r\n","# command line arguments --> file name, video_file_name, gpu or cpu \r\n","\r\n","\r\n","# first decompose the video to frames\r\n","# place the video to be detected into the directory \r\n","\r\n","video = sys.argv[1]\r\n","pred_text_file_name = sys.argv[2]\r\n","\r\n","\r\n","text_file = 'frames.txt'\r\n","\r\n","print('decomposing video to frames this may take a while  for large videos :) .....')\r\n","frames_path = 'video_frames/'\r\n","os.makedirs('video_frames/', exist_ok=True)\r\n","os.makedirs('predictions/', exist_ok=True)\r\n","\r\n","vid = VideoFileClip(video)\r\n","vid = six_four_crop_video(vid)\r\n","\r\n","frames = [frame for frame in vid.iter_frames()]\r\n","\r\n","f = open(text_file, 'w+')\r\n","\r\n","for j, frame in enumerate(frames):\r\n","        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\r\n","        im = Image.fromarray(frame)\r\n","        im.save(frame_path)            \r\n","        f.write(frame_path + '\\n')    \r\n","\r\n","print('frame decomposition complete !!! ')\r\n","\r\n","device = 'cuda'\r\n","\r\n","#load model\r\n","model = TransitionCNN()\r\n","model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\r\n","model.to(device)\r\n","\r\n","prediction_text_file = 'predictions/' + pred_text_file_name \r\n","\r\n","pred_file = open(prediction_text_file, 'w+')\r\n","\r\n","print('computing predictions for video', video, '...................' )\r\n","\r\n","test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\r\n","test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\r\n","\r\n","video_indexes = []\r\n","vals = np.arange(test_video.get_line_number())\r\n","length = len(test_video)\r\n","\r\n","for val in range(length):\r\n","    s,e = return_start_and_end(val)\r\n","    video_indexes.append(vals[s:e])\r\n","\r\n","for indx, batch in enumerate(test_loader):\r\n","        batch.to(device)\r\n","        batch = batch.type('torch.cuda.FloatTensor')\r\n","        predictions = model(batch)\r\n","        predictions = predictions.argmax(dim=1).cpu().numpy()\r\n","        for idx, prediction_set in enumerate(predictions):\r\n","            for i, prediction in enumerate(prediction_set):\r\n","                if prediction[0][0] == 0:\r\n","                    frame_index = video_indexes[indx][i+5]\r\n","                    #print(frame_index)\r\n","                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\r\n","                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\r\n","                    #print(frame_path0)\r\n","                    # read images\r\n","                    img1 = cv2.imread(frame_path0)  \r\n","                    img2 = cv2.imread(frame_path) \r\n","\r\n","                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\r\n","                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\r\n","\r\n","                    #sift\r\n","                    sift = cv2.xfeatures2d.SIFT_create()\r\n","\r\n","                    keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\r\n","                    keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\r\n","                    a = max()\r\n","                    print(len(keypoints_1))\r\n","                    #feature matching\r\n","                    bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\r\n","\r\n","                    matches = bf.match(descriptors_1,descriptors_2)\r\n","                    #print(len(matches))\r\n","                    #matches = sorted(matches, key = lambda x:x.distance)\r\n","                    #print(len(matches))\r\n","                    print(len(matches)/len(keypoints_1))\r\n","                    \r\n","\r\n","\r\n","\r\n","\r\n","                    \r\n","                    pred_file.write(str(frame_index) + '\\n')\r\n","pred_file.close()\r\n","\r\n","# delete files used for process\r\n","os.remove('frames.txt')\r\n","shutil.rmtree('video_frames/')\r\n","\r\n","print('Predictions complete !!!')\r\n","print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMFcWdvKSEh8"},"source":["!python get_transition_frames_gpu.py /content/v1.mp4 pred1.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqjWTy3J-z9f"},"source":["#using SURF features\r\n","!python get_transition_frames_gpu.py /content/v3.mp4 pred2.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"decEU4FOZXE-"},"source":["import cv2 \r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","fr0 = '/content/messi_3.jfif'\r\n","fr1 = '/content/messi_4.jfif'\r\n","# read images\r\n","img1 = cv2.imread(fr0)  \r\n","img2 = cv2.imread(fr1) \r\n","\r\n","img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\r\n","img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\r\n","\r\n","#sift\r\n","sift = cv2.xfeatures2d.SIFT_create()\r\n","\r\n","keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\r\n","keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\r\n","print(len(keypoints_1))\r\n","print(len(keypoints_2))\r\n","#feature matching\r\n","bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\r\n","\r\n","matches = bf.match(descriptors_1,descriptors_2)\r\n","\r\n","print(len(matches))\r\n","\r\n","print(len(matches)/len(keypoints_1))\r\n","\r\n","#img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:50], img2, flags=2)\r\n","#plt.imshow(img3),plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7EsjyoRt13J"},"source":["#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL KEYPOINT TESTING\r\n","import sys\r\n","import shutil\r\n","\r\n","import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from torchvision import transforms, utils\r\n","import numpy as np\r\n","from snippet import getSnippet\r\n","from math import floor\r\n","from transition_network import TransitionCNN\r\n","from utilities import normalize_frame, print_shape\r\n","import pandas as pd\r\n","import os\r\n","import time\r\n","from TestVideo import TestVideo, return_start_and_end\r\n","from moviepy.editor import VideoFileClip\r\n","from video_processing import six_four_crop_video\r\n","from PIL import Image\r\n","import cv2 \r\n","#import matplotlib.pyplot as plt\r\n","#%matplotlib inline\r\n","\r\n","# command line arguments --> file name, video_file_name, gpu or cpu \r\n","\r\n","\r\n","# first decompose the video to frames\r\n","# place the video to be detected into the directory \r\n","\r\n","video = sys.argv[1]\r\n","pred_text_file_name = sys.argv[2]\r\n","\r\n","frames_path = '/content/shot_boudary_detector/video_frames/'\r\n","\r\n","\r\n","text_file = 'frames.txt'\r\n","\r\n","\r\n","device = 'cuda'\r\n","\r\n","#load model\r\n","model = TransitionCNN()\r\n","model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\r\n","model.to(device)\r\n","\r\n","prediction_text_file = 'predictions/' + pred_text_file_name \r\n","\r\n","pred_file = open(prediction_text_file, 'w+')\r\n","\r\n","print('computing predictions for video', video, '...................' )\r\n","\r\n","test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\r\n","test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\r\n","\r\n","video_indexes = []\r\n","vals = np.arange(test_video.get_line_number())\r\n","length = len(test_video)\r\n","\r\n","for val in range(length):\r\n","    s,e = return_start_and_end(val)\r\n","    video_indexes.append(vals[s:e])\r\n","\r\n","for indx, batch in enumerate(test_loader):\r\n","        batch.to(device)\r\n","        batch = batch.type('torch.cuda.FloatTensor')\r\n","        predictions = model(batch)\r\n","        predictions = predictions.argmax(dim=1).cpu().numpy()\r\n","        for idx, prediction_set in enumerate(predictions):\r\n","            for i, prediction in enumerate(prediction_set):\r\n","                if prediction[0][0] == 0:\r\n","                    frame_index = video_indexes[indx][i+5]\r\n","                    #print(frame_index)\r\n","                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\r\n","                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\r\n","                    #print(frame_path0)\r\n","                    # read images\r\n","                    img1 = cv2.imread(frame_path0)  \r\n","                    img2 = cv2.imread(frame_path) \r\n","\r\n","                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\r\n","                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\r\n","\r\n","                    #sift\r\n","                    sift = cv2.xfeatures2d.SIFT_create()\r\n","                    #BRISK\r\n","                    brisk = cv2.BRISK_create()\r\n","\r\n","                    keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\r\n","                    keypoints_2, descriptors_2 = brisk.detectAndCompute(img2,None)\r\n","\r\n","                    print(len(keypoints_2))\r\n","                    #feature matching\r\n","                    #bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\r\n","\r\n","                    #matches = bf.match(descriptors_1,descriptors_2)\r\n","                    #print(len(matches))\r\n","                    #matches = sorted(matches, key = lambda x:x.distance)\r\n","                    #print(len(matches))\r\n","                    #print(len(matches)/len(keypoints_1))\r\n","                    \r\n","\r\n","\r\n","\r\n","\r\n","                    \r\n","                    pred_file.write(str(frame_index) + '\\n')\r\n","pred_file.close()\r\n","\r\n","# delete files used for process\r\n","os.remove('frames.txt')\r\n","shutil.rmtree('video_frames/')\r\n","\r\n","print('Predictions complete !!!')\r\n","print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVJqttoVbqIJ"},"source":["import cv2\r\n","img1 = cv2.imread('/content/messi_3.jfif')  \r\n","img2 = cv2.imread('/content/messi_4.jfif') \r\n","\r\n","img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\r\n","img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\r\n","\r\n","                    #sift\r\n","sift = cv2.xfeatures2d.SIFT_create()\r\n","\r\n","keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\r\n","keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\r\n","\r\n","print(len(keypoints_1))\r\n","                    #feature matching\r\n","bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\r\n","\r\n","matches = bf.match(descriptors_1,descriptors_2)\r\n","print(len(matches))\r\n","                    #matches = sorted(matches, key = lambda x:x.distance)\r\n","                    #print(len(matches))\r\n","                    #print(len(matches)/len(keypoints_1))\r\n","                    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcEfJOIY0bdx"},"source":["#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL\r\n","import sys\r\n","import shutil\r\n","\r\n","import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from torchvision import transforms, utils\r\n","import numpy as np\r\n","from snippet import getSnippet\r\n","from math import floor\r\n","from transition_network import TransitionCNN\r\n","from utilities import normalize_frame, print_shape\r\n","import pandas as pd\r\n","import os\r\n","import time\r\n","from TestVideo import TestVideo, return_start_and_end\r\n","from moviepy.editor import VideoFileClip\r\n","from video_processing import six_four_crop_video\r\n","from PIL import Image\r\n","import cv2 \r\n","#import matplotlib.pyplot as plt\r\n","#%matplotlib inline\r\n","\r\n","# command line arguments --> file name, video_file_name, gpu or cpu \r\n","\r\n","\r\n","# first decompose the video to frames\r\n","# place the video to be detected into the directory \r\n","\r\n","video = sys.argv[1]\r\n","pred_text_file_name = sys.argv[2]\r\n","\r\n","\r\n","text_file = 'frames.txt'\r\n","\r\n","print('decomposing video to frames this may take a while  for large videos :) .....')\r\n","frames_path = 'video_frames/'\r\n","os.makedirs('video_frames/', exist_ok=True)\r\n","os.makedirs('predictions/', exist_ok=True)\r\n","\r\n","vid = VideoFileClip(video)\r\n","vid = six_four_crop_video(vid)\r\n","\r\n","frames = [frame for frame in vid.iter_frames()]\r\n","\r\n","f = open(text_file, 'w+')\r\n","\r\n","for j, frame in enumerate(frames):\r\n","        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\r\n","        im = Image.fromarray(frame)\r\n","        im.save(frame_path)            \r\n","        f.write(frame_path + '\\n')    \r\n","\r\n","print('frame decomposition complete !!! ')\r\n","\r\n","device = 'cuda'\r\n","\r\n","#load model\r\n","model = TransitionCNN()\r\n","model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\r\n","model.to(device)\r\n","\r\n","prediction_text_file = 'predictions/' + pred_text_file_name \r\n","\r\n","pred_file = open(prediction_text_file, 'w+')\r\n","\r\n","print('computing predictions for video', video, '...................' )\r\n","\r\n","test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\r\n","test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\r\n","\r\n","video_indexes = []\r\n","vals = np.arange(test_video.get_line_number())\r\n","length = len(test_video)\r\n","\r\n","for val in range(length):\r\n","    s,e = return_start_and_end(val)\r\n","    video_indexes.append(vals[s:e])\r\n","\r\n","for indx, batch in enumerate(test_loader):\r\n","        batch.to(device)\r\n","        batch = batch.type('torch.cuda.FloatTensor')\r\n","        predictions = model(batch)\r\n","        predictions = predictions.argmax(dim=1).cpu().numpy()\r\n","        for idx, prediction_set in enumerate(predictions):\r\n","            for i, prediction in enumerate(prediction_set):\r\n","                if prediction[0][0] == 0:\r\n","                    frame_index = video_indexes[indx][i+5]\r\n","                    #print(frame_index)\r\n","                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\r\n","                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\r\n","                    #print(frame_path0)\r\n","                    # read images\r\n","                    img1 = cv2.imread(frame_path0)  \r\n","                    img2 = cv2.imread(frame_path) \r\n","\r\n","                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\r\n","                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\r\n","\r\n","                    #sift\r\n","                    #sift = cv2.xfeatures2d.SIFT_create()\r\n","                    surf = cv2.xfeatures2d.SURF_create()\r\n","\r\n","                    #keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\r\n","                    keypoints_2, descriptors_2 = surf.detectAndCompute(img2,None)\r\n","                    \r\n","                    print(len(keypoints_2))\r\n","                    #feature matching\r\n","                    #bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\r\n","\r\n","                    #matches = bf.match(descriptors_1,descriptors_2)\r\n","                    #print(len(matches))\r\n","                    #matches = sorted(matches, key = lambda x:x.distance)\r\n","                    #print(len(matches))\r\n","                    #print(len(matches)/len(keypoints_1))\r\n","                    if (len(keypoints_2) != 0):               \r\n","                      pred_file.write(str(frame_index) + '\\n')\r\n","                    \r\n","pred_file.close()\r\n","\r\n","# delete files used for process\r\n","os.remove('frames.txt')\r\n","shutil.rmtree('video_frames/')\r\n","\r\n","print('Predictions complete !!!')\r\n","print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAYsprsZQyri"},"source":["#BRISK KEYPOINTS\r\n","!python get_transition_frames_gpu.py /content/v10.mp4 pred2.txt"],"execution_count":null,"outputs":[]}]}